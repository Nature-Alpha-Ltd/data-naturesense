{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "from configparser import ConfigParser\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from utils.gcp_tools import (\n",
    "    get_git_branch,\n",
    "    last_day_of_month,\n",
    "    run_query,\n",
    "    save_results,\n",
    "    write_df_to_bq,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Read your config.ini\n",
    "config = ConfigParser()\n",
    "config.read(\"config.ini\")\n",
    "\n",
    "# 2. Grab the section name from the ENVIRONMENT env‑var\n",
    "ENVIRONMENT = os.environ.get(\"ENVIRONMENT\", \"dev\")\n",
    "\n",
    "# 3. Fetch that section (this yields a SectionProxy)\n",
    "cfg = config[ENVIRONMENT]\n",
    "\n",
    "PROJECT_ID = cfg[\"PROJECT_ID\"]\n",
    "BQ_DATASET = cfg[\"BQ_DATASET\"]\n",
    "ALD = cfg[\"ALD\"]\n",
    "ASSET_COUNTS_GUESTIMATES = cfg[\"ASSET_COUNTS_GUESTIMATES\"]\n",
    "NATURESENSE_COUNTRY = cfg[\"NATURESENSE_COUNTRY\"]\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data() -> tuple:\n",
    "    \"\"\"Load all required data from BigQuery using consistently formed SQL queries.\"\"\"\n",
    "    # ALD\n",
    "    query_ald = f\"\"\"\n",
    "    SELECT \n",
    "        na_entity_id, \n",
    "        entity_isin, \n",
    "        entity_name,\n",
    "        priority_asset,\n",
    "        asset_type_id,\n",
    "        sensitive_locations, \n",
    "        biodiversity_importance, \n",
    "        high_ecosystem_integrity, \n",
    "        decline_in_ecosystem_integrity,\n",
    "        physical_water_risk, \n",
    "        ecosystem_services_provision_importance, \n",
    "        proximity_to_protected_areas, \n",
    "        proximity_to_kbas,\n",
    "        species_rarity_weighted_richness, \n",
    "        species_threat_abatement, \n",
    "        species_threat_abatement_marine, \n",
    "        proximity_to_mangroves,\n",
    "        ecosystem_intactness_index, \n",
    "        biodiversity_intactness_index, \n",
    "        ocean_health_index, \n",
    "        trend_in_ecosystem_intactness_index,\n",
    "        deforestation_hotspots, \n",
    "        water_availability, \n",
    "        water_pollution, \n",
    "        drought, \n",
    "        riverine_flood, \n",
    "        coastal_flood, \n",
    "        cumulative_impact_on_oceans, \n",
    "        critical_areas_for_biodiversity_and_ncp, \n",
    "        areas_of_importance_for_biodiversity_and_climate,\n",
    "        in_water_scarcity\n",
    "    FROM {ALD};\n",
    "    \"\"\"\n",
    "    logging.info(\"Loading data from %s\", ALD)\n",
    "    ald = run_query(query_ald)\n",
    "\n",
    "    # Asset counts guestimates\n",
    "    query_assets_guestimates = f\"\"\"\n",
    "    SELECT\n",
    "        *\n",
    "    FROM {ASSET_COUNTS_GUESTIMATES};\n",
    "    \"\"\"\n",
    "    logging.info(\"Loading data from %s\", ASSET_COUNTS_GUESTIMATES)\n",
    "    assets_guestimates = run_query(query_assets_guestimates)\n",
    "\n",
    "    # NatureSense country level\n",
    "    query_ns_country = f\"\"\"\n",
    "    SELECT\n",
    "        *\n",
    "    FROM {NATURESENSE_COUNTRY};\n",
    "    \"\"\"\n",
    "    logging.info(\"Loading data from %s\", NATURESENSE_COUNTRY)\n",
    "    naturesense_country = run_query(query_ns_country)\n",
    "\n",
    "    return ald, assets_guestimates, naturesense_country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loading data from na-datalake.production_ready_access_layer.naturesense_solved_assets\n",
      "INFO:root:Using local service account: na-datalake-6f5a94599e1b 1.json\n",
      "INFO:root:Loading data from na-datalake.production_ready_access_layer.guestimator_latest\n",
      "INFO:root:Loading data from na-datalake.production_ready_access_layer.naturesense_country_level\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "ald, assets_guestimates, naturesense_country = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate companies evidences, i.e., aggregate ALD to company\n",
    "ald[\"material_asset\"] = ~ald[\"asset_type_id\"].isin([11, 12]).astype(bool)\n",
    "# ald[\"in_water_scarcity\"] = (\n",
    "#     (ald[\"water_availability\"] > 0.6) & (ald[\"material_asset\"] == True)\n",
    "# ).astype(bool\n",
    "\n",
    "ald_counts = (\n",
    "    ald.groupby(\"na_entity_id\")\n",
    "    .agg(\n",
    "        assets_count=(\"na_entity_id\", \"count\"),\n",
    "        priority_assets_count=(\"priority_asset\", \"sum\"),\n",
    "        material_assets_count=(\"material_asset\", \"sum\"),\n",
    "        in_water_scarcity_count=(\"in_water_scarcity\", \"sum\"),\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "ald_counts[\"priority_assets_percentage\"] = round(\n",
    "    (ald_counts[\"priority_assets_count\"] / ald_counts[\"assets_count\"]) * 100, 3\n",
    ")\n",
    "\n",
    "ald_counts[\"in_water_scarcity_percentage\"] = round(\n",
    "    (ald_counts[\"in_water_scarcity_count\"] / ald_counts[\"assets_count\"]) * 100, 3\n",
    ")\n",
    "\n",
    "ald_subset = ald[ald[\"material_asset\"] == True]\n",
    "naturesense_metrics = [\n",
    "    \"sensitive_locations\",\n",
    "    \"biodiversity_importance\",\n",
    "    \"high_ecosystem_integrity\",\n",
    "    \"decline_in_ecosystem_integrity\",\n",
    "    \"physical_water_risk\",\n",
    "    \"ecosystem_services_provision_importance\",\n",
    "    \"proximity_to_protected_areas\",\n",
    "    \"proximity_to_kbas\",\n",
    "    \"species_rarity_weighted_richness\",\n",
    "    \"species_threat_abatement\",\n",
    "    \"species_threat_abatement_marine\",\n",
    "    \"proximity_to_mangroves\",\n",
    "    \"ecosystem_intactness_index\",\n",
    "    \"biodiversity_intactness_index\",\n",
    "    \"ocean_health_index\",\n",
    "    \"trend_in_ecosystem_intactness_index\",\n",
    "    \"deforestation_hotspots\",\n",
    "    \"water_availability\",\n",
    "    \"water_pollution\",\n",
    "    \"drought\",\n",
    "    \"riverine_flood\",\n",
    "    \"coastal_flood\",\n",
    "    \"cumulative_impact_on_oceans\",\n",
    "    \"critical_areas_for_biodiversity_and_ncp\",\n",
    "    \"areas_of_importance_for_biodiversity_and_climate\",\n",
    "    ]\n",
    "\n",
    "ald_averages = (\n",
    "    ald_subset.groupby(\"na_entity_id\")\n",
    "    .agg(\n",
    "        **{\n",
    "            f\"{col}\": (col, lambda x: round(x.mean(skipna=True), 3))\n",
    "            for col in naturesense_metrics\n",
    "        }\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "companies_evidences = ald_counts.merge(\n",
    "    ald_averages, on=\"na_entity_id\", how=\"left\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Country_priors.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import logging\n",
    "import re\n",
    "from typing import List, Union, Tuple\n",
    "from tqdm import tqdm\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.ERROR, format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_posterior(\n",
    "    evidence: Union[pd.Series, list, float, int],\n",
    "    prior: float,\n",
    "    sample_size: Union[pd.Series, list, float, int],\n",
    "    k: Union[float, int] = 10,\n",
    ") -> Union[pd.Series, float, None]:\n",
    "    \"\"\"\n",
    "    Compute posterior scores by combining entity-specific evidence with a global prior\n",
    "    based on sample size.\n",
    "    \"\"\"\n",
    "    logger.info(\"Starting posterior computation\")\n",
    "    try:\n",
    "        # Ensure evidence and sample_size are converted to NumPy arrays for consistency\n",
    "        if isinstance(evidence, (float, int)):\n",
    "            evidence = np.array([evidence], dtype=float)\n",
    "        elif isinstance(evidence, list):\n",
    "            evidence = np.array(evidence, dtype=float)\n",
    "        elif isinstance(evidence, pd.Series):\n",
    "            evidence = evidence.to_numpy(dtype=float)\n",
    "        elif isinstance(evidence, (np.ndarray, np.int64, np.float64)):\n",
    "            evidence = np.array(evidence, dtype=float)\n",
    "\n",
    "        else:\n",
    "            logger.error(f\"Invalid type for evidence: {type(evidence)}\")\n",
    "            return None\n",
    "\n",
    "        if isinstance(sample_size, (float, int)):\n",
    "            sample_size = np.array([sample_size], dtype=float)\n",
    "        elif isinstance(sample_size, list):\n",
    "            sample_size = np.array(sample_size, dtype=float)\n",
    "        elif isinstance(sample_size, pd.Series):\n",
    "            sample_size = sample_size.to_numpy(dtype=float)\n",
    "        elif isinstance(sample_size, (np.ndarray, np.int64, np.float64)):\n",
    "            sample_size = np.array(sample_size, dtype=float)\n",
    "        else:\n",
    "            logger.error(f\"Invalid type for sample_size: {type(sample_size)}\")\n",
    "            return None\n",
    "\n",
    "        if not isinstance(k, (int, float)) or k < 0:\n",
    "            logger.error(\"k must be a positive number\")\n",
    "            return None\n",
    "\n",
    "        # Convert inputs to NumPy arrays for consistency\n",
    "        evidence = np.atleast_1d(np.asarray(evidence, dtype=float))\n",
    "        sample_size = np.atleast_1d(np.asarray(sample_size, dtype=float))\n",
    "\n",
    "        # Handle edge cases for arrays\n",
    "        if k == 0:\n",
    "            if np.all(sample_size == 0):\n",
    "                # Return zeros with same shape as input\n",
    "                zeros = np.zeros_like(evidence)\n",
    "                return float(zeros[0]) if zeros.size == 1 else pd.Series(zeros)\n",
    "            return float(evidence[0]) if evidence.size == 1 else pd.Series(evidence)\n",
    "        elif np.all(sample_size == 0):\n",
    "            # Return prior with same shape as input\n",
    "            priors = np.full_like(evidence, prior)\n",
    "            return float(priors[0]) if priors.size == 1 else pd.Series(priors)\n",
    "\n",
    "        # Compute weights safely, avoiding NaN by ensuring effective_k is never zero\n",
    "        adapted_k = np.minimum(sample_size / k, 1)\n",
    "        w_i = np.where(sample_size == 0, 1, adapted_k)\n",
    "\n",
    "        # Compute posterior\n",
    "        theta_i = w_i * evidence + (1 - w_i) * prior\n",
    "\n",
    "        # Return a float if the input was scalar, else return a Series\n",
    "        return float(theta_i[0]) if theta_i.size == 1 else pd.Series(theta_i)\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during posterior computation: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def get_company_countries(company_row: pd.Series) -> List[str]:\n",
    "    \"\"\"\n",
    "    Extract valid ISO country codes from company row where asset count > 0.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    company_row : pd.Series\n",
    "        Row containing company's country distribution\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List[str]\n",
    "        List of valid ISO country codes with positive asset counts\n",
    "    \"\"\"\n",
    "    iso_pattern = re.compile(r\"^[A-Z0-9]{3}$\")\n",
    "    return [c for c in company_row.index if iso_pattern.match(c) and company_row[c] > 0]\n",
    "\n",
    "\n",
    "def calculate_country_prior(\n",
    "    company_row: pd.Series,\n",
    "    country_priors: pd.DataFrame,\n",
    "    evidence_columns: Union[str, List[str]],\n",
    "    country_codes: List[str],\n",
    "    isin: str,\n",
    ") -> List[float]:\n",
    "    \"\"\"\n",
    "    Calculate the country-weighted prior for a company for one or multiple geospatial columns.\n",
    "    Uses vectorized operations to compute weighted averages for all evidence columns at once.\n",
    "    Returns None for each column if there are no valid weights.\n",
    "    \"\"\"\n",
    "    # Ensure evidence_columns is a list\n",
    "    if isinstance(evidence_columns, str):\n",
    "        evidence_columns = [evidence_columns]\n",
    "\n",
    "    # Obtain guestimated countries with company presence\n",
    "    company_countries = get_company_countries(company_row)\n",
    "\n",
    "    # Check which countries have country avg prior available\n",
    "    available_countries = [c for c in company_countries if c in country_codes]\n",
    "    missing_countries = set(company_countries) - set(country_codes)\n",
    "\n",
    "    if missing_countries:\n",
    "        logger.info(\n",
    "            f\"ISIN {isin} has assets in countries missing from priors: {missing_countries}. \"\n",
    "            \"These will be excluded from the weighted average calculation.\"\n",
    "        )\n",
    "\n",
    "    if company_row.empty:\n",
    "        logger.warning(f\"No country distribution found for ISIN {isin}\")\n",
    "        return [None] * len(evidence_columns)\n",
    "\n",
    "    # Get the priors matrix for available countries and evidence columns\n",
    "    priors_matrix = country_priors[\n",
    "        country_priors[\"country_code\"].isin(available_countries)\n",
    "    ]\n",
    "    priors_matrix = priors_matrix.set_index(\"country_code\")[evidence_columns]\n",
    "\n",
    "    # Create weights array for available countries\n",
    "    weights = pd.Series(\n",
    "        {country: company_row[country] for country in available_countries}\n",
    "    )\n",
    "\n",
    "    # If no weights or all weights are zero, return None for each column\n",
    "    if not available_countries or weights.sum() == 0:\n",
    "        return [None] * len(evidence_columns)\n",
    "\n",
    "    # Normalize weights and ensure index alignment\n",
    "    weights = weights / weights.sum()\n",
    "    weights = weights.reindex(priors_matrix.index)\n",
    "\n",
    "    # Compute weighted average for all columns at once using matrix multiplication\n",
    "    weighted_priors = weights.dot(priors_matrix)\n",
    "\n",
    "    # Convert to list and handle any NaN values\n",
    "    return [float(val) if pd.notnull(val) else None for val in weighted_priors]\n",
    "\n",
    "\n",
    "def calculate_effective_k(k: float, total_company_locations: int, company_assets: int) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the effective k value based on various scenarios.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    k : float\n",
    "        Original k value. If < 1, interpreted as proportion of total_company_locations\n",
    "    total_company_locations : int\n",
    "        Estimated total number of company locations in the world\n",
    "    company_assets : int\n",
    "        Actual number of discovered company assets\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Adjusted k value\n",
    "    \"\"\"\n",
    "    # If k is proportional, convert to absolute number\n",
    "    # Ensure effective_k is at least 1 if total_company_locations is greater than 0\n",
    "    if k < 1:\n",
    "        effective_k = max(1, np.ceil(k * total_company_locations)) if total_company_locations > 0 else 0\n",
    "    else:\n",
    "        effective_k = k\n",
    "\n",
    "    # If we found more assets than estimated locations, update our estimate\n",
    "    if total_company_locations != 0 and company_assets > total_company_locations:\n",
    "        total_company_locations = company_assets\n",
    "        \n",
    "    # If estimated total locations is less than k, adjust k down\n",
    "    if total_company_locations < effective_k:\n",
    "        effective_k = total_company_locations\n",
    "\n",
    "    return float(effective_k)\n",
    "\n",
    "\n",
    "def no_guestimates_adjust_priors_and_k(weighted_priors: Union[List[float], None], effective_k: float, k: float, default_prior: float = 0.542) -> Tuple[List[float], float]:\n",
    "    \"\"\"\n",
    "    Adjust priors and effective_k for companies with no prior and few assets.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    weighted_priors : Union[List[float], None]\n",
    "        List of weighted priors, may contain None values, or be None itself\n",
    "    effective_k : float\n",
    "        Current effective k value\n",
    "    k : float\n",
    "        Original k value\n",
    "    default_prior : float, optional\n",
    "        Default prior to use when no prior exists, by default 0.542\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[List[float], float]\n",
    "        Adjusted weighted priors and effective k value\n",
    "    \"\"\"\n",
    "    if weighted_priors is None or any(p is None for p in weighted_priors):\n",
    "        if effective_k < k:\n",
    "            if weighted_priors is None:\n",
    "                weighted_priors = [default_prior]\n",
    "            else:\n",
    "                weighted_priors = [default_prior if p is None else p for p in weighted_priors]\n",
    "            effective_k = k\n",
    "    return weighted_priors, effective_k\n",
    "\n",
    "\n",
    "def process_company_evidence(\n",
    "    company_data: pd.DataFrame,\n",
    "    country_dist: pd.DataFrame,\n",
    "    country_priors: pd.DataFrame,\n",
    "    evidence_columns: List[str],\n",
    "    k: Union[int, float] = 20,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Process company evidence using country distribution and priors.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    company_data : pd.DataFrame\n",
    "        Company-level data containing:\n",
    "        - na_entity_id: Company identifier\n",
    "        - assets_count: Number of total assets\n",
    "        - evidence_columns: Columns containing evidence to process\n",
    "    country_dist : pd.DataFrame\n",
    "        Country distribution data containing:\n",
    "        - na_entity_id: Company identifier\n",
    "        - total_company_locations: Optional total locations count\n",
    "        - One column per country code with number of assets\n",
    "    country_priors : pd.DataFrame\n",
    "        Country-level priors containing:\n",
    "        - country_code: ISO alpha-3 country code\n",
    "        - columns matching evidence_columns with prior values\n",
    "    evidence_columns : List[str]\n",
    "        List of column names in company_data to process\n",
    "    k : int or float, optional\n",
    "        If k > 1: interpreted as absolute number of asset locations\n",
    "        If k ≤ 1: interpreted as proportion of company's total locations\n",
    "        Default is 20.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame with original company data plus new posterior columns\n",
    "    \"\"\"\n",
    "    logger.info(\"Starting company evidence processing\")\n",
    "\n",
    "    try:\n",
    "        # Validate required columns\n",
    "        required_company_cols = [\"na_entity_id\", \"material_assets_count\"]  # + evidence_columns\n",
    "        if not all(col in company_data.columns for col in required_company_cols):\n",
    "            missing_cols = set(required_company_cols) - set(company_data.columns)\n",
    "            raise ValueError(\n",
    "                f\"Missing required columns in company_data: {missing_cols}\"\n",
    "            )\n",
    "        # entity in guestimator file\n",
    "        if \"na_entity_id\" not in country_dist.columns:\n",
    "            raise ValueError(\"country_dist must contain 'na_entity_id' column\")\n",
    "\n",
    "        # Get country codes from country_priors\n",
    "        country_codes = country_priors[\"country_code\"].tolist()\n",
    "\n",
    "        # This will take the sum of the assets listed under each country code. \n",
    "        # This is the total number of assets for the company.\n",
    "        country_dist_copy = country_dist.copy()\n",
    "\n",
    "        if True: #\"total_company_locations\" not in country_dist_copy.columns:\n",
    "            # country_cols = [\n",
    "            #     col\n",
    "            #     for col in country_dist_copy.columns\n",
    "            #     if col != \"na_entity_id\" and col != \"total_company_locations\"\n",
    "            # ]\n",
    "            country_cols = [col for col in country_dist.columns if len(col) == 3]\n",
    "\n",
    "            country_assets_sum = country_dist_copy[country_cols].sum(axis=1)\n",
    "            country_dist_copy[\"total_company_locations\"] = country_assets_sum\n",
    "\n",
    "\n",
    "        # Create output DataFrame\n",
    "        result_df = company_data.copy()\n",
    "\n",
    "        # Check which evidence columns are available in country_priors\n",
    "        available_evidence = [\n",
    "            col for col in evidence_columns if col in country_priors.columns\n",
    "        ]\n",
    "        missing_evidence = set(evidence_columns) - set(available_evidence)\n",
    "\n",
    "        if missing_evidence:\n",
    "            logger.warning(\n",
    "                f\"Following evidence columns not found in country_priors: {missing_evidence}\"\n",
    "            )\n",
    "            evidence_columns = available_evidence\n",
    "\n",
    "        if not evidence_columns:\n",
    "            logger.error(\"No valid evidence columns to process\")\n",
    "            return result_df\n",
    "\n",
    "        # Keep track of missing ISINs\n",
    "        missing_isins = []\n",
    "\n",
    "        # Add posterior columns to result_df, initialized as copies of original columns\n",
    "        for col in evidence_columns:\n",
    "            result_df[f\"{col}_posterior\"] = result_df[col]\n",
    "\n",
    "        # Process each company\n",
    "        for isin in tqdm(\n",
    "            result_df[\"na_entity_id\"].unique(), desc=\"Implementing NatureSense Priors\"\n",
    "        ):\n",
    "            # Initialize total_locations and company_assets\n",
    "            total_locations = 0\n",
    "            company_assets = int(company_data.loc[\n",
    "                company_data[\"na_entity_id\"] == isin, \"material_assets_count\"\n",
    "            ].iloc[0])\n",
    "            # Get evidence values for all columns - do this for all companies\n",
    "            company_evidence = result_df.loc[\n",
    "                result_df[\"na_entity_id\"] == isin, evidence_columns\n",
    "            ].iloc[0]\n",
    "\n",
    "            if isin not in country_dist_copy[\"na_entity_id\"].values:\n",
    "                missing_isins.append(isin)\n",
    "                weighted_priors = [None] * len(evidence_columns)  # Initialize with correct length\n",
    "            else:\n",
    "                # Get company evidence and asset count for this column\n",
    "                company_row = country_dist_copy[\n",
    "                    country_dist_copy[\"na_entity_id\"] == isin\n",
    "                ].iloc[0]\n",
    "                total_locations = company_row[\"total_company_locations\"]\n",
    " \n",
    "                # If both assets_count and total_company_locations are 0, set all evidence columns to None\n",
    "                if company_assets == 0 and total_locations == 0:\n",
    "                    for col in evidence_columns:\n",
    "                        result_df.loc[result_df[\"na_entity_id\"] == isin, f\"{col}_posterior\"] = np.nan\n",
    "                    continue\n",
    "\n",
    "\n",
    "                # Get all weighted priors at once\n",
    "                weighted_priors = calculate_country_prior(\n",
    "                    company_row=company_row,\n",
    "                    country_priors=country_priors,\n",
    "                    evidence_columns=evidence_columns,\n",
    "                    country_codes=country_codes,\n",
    "                    isin=isin,\n",
    "                )\n",
    "            # Calculate effective k\n",
    "            effective_k = calculate_effective_k(\n",
    "                k=k,\n",
    "                total_company_locations=total_locations,\n",
    "                company_assets=company_assets\n",
    "            )\n",
    "            # print(\"XXXXXXXXX total locations\", total_locations, \"company assets\", company_assets, \"effective k\", effective_k)\n",
    "\n",
    "            # Adjust priors and k if necessary\n",
    "            weighted_priors, effective_k = no_guestimates_adjust_priors_and_k(weighted_priors, effective_k, k)\n",
    "            # print(f\"ZZZZZZZZZ Weighted priors are {weighted_priors}, effective k is {effective_k}\")\n",
    "            # Now weighted_priors should never be None, but its elements might be\n",
    "            for idx, col in enumerate(evidence_columns):\n",
    "                if weighted_priors[idx] is None:  # Check individual prior\n",
    "                    continue\n",
    "\n",
    "                # Get evidence value for this specific column\n",
    "                evidence_value = float(company_evidence[col])\n",
    "\n",
    "\n",
    "                # Compute posterior using the corresponding prior\n",
    "                posterior = compute_posterior(\n",
    "                    evidence=evidence_value,\n",
    "                    prior=float(weighted_priors[idx]),\n",
    "                    sample_size=company_assets,\n",
    "                    k=effective_k,\n",
    "                )\n",
    "                # Store result\n",
    "                result_df.loc[result_df[\"na_entity_id\"] == isin, f\"{col}_posterior\"] = posterior\n",
    "\n",
    "        if missing_isins:\n",
    "            logger.warning(\n",
    "                f\"{len(missing_isins)} companies were not found in country distribution data\"\n",
    "            )\n",
    "\n",
    "        logger.info(\"Completed company evidence processing\")\n",
    "        return result_df\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in process_company_evidence: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "\n",
    "def process_naturesense_data(ns_df, core_wide, country_dist, country_priors, k=20, \n",
    "                            nature_sense_evidence_columns=None):\n",
    "    \"\"\"\n",
    "    Processes NatureSense data by validating columns, merging country priors,\n",
    "    and calculating sensitive location probabilities.\n",
    "\n",
    "    Parameters:\n",
    "    - ns_df (pd.DataFrame): The raw NatureSense data.\n",
    "    - core_wide (pd.DataFrame): The core dataset containing entity ISINs.\n",
    "    - country_dist (pd.DataFrame): table with country distribution data for each company\n",
    "    - country_priors (pd.DataFrame): Country-level prior data\n",
    "    - k (int): Parameter for Bayesian updating\n",
    "    - nature_sense_evidence_columns (list, optional): List of evidence columns to process.\n",
    "      If None, uses the default naturesense_metrics.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: Processed ns_df ready for nature risk calculations.\n",
    "    \"\"\"\n",
    "    # Filter to only include entities present in core_wide\n",
    "    ns_df = ns_df.loc[ns_df[\"na_entity_id\"].isin(core_wide[\"na_entity_id\"])]\n",
    "    print(\"shape non nans beginning\",ns_df[~ns_df[\"sensitive_locations\"].isna()].shape, 'total rows', ns_df.shape[0])\n",
    "\n",
    "    # Use default columns if none provided\n",
    "    if nature_sense_evidence_columns is None:\n",
    "        nature_sense_evidence_columns = naturesense_metrics\n",
    "\n",
    "    # Define required columns\n",
    "    identifier_columns = [\"na_entity_id\", \"material_assets_count\"]\n",
    "    required_columns = identifier_columns + nature_sense_evidence_columns\n",
    "\n",
    "    # Check for missing columns in NatureSense data\n",
    "    missing_columns = [col for col in required_columns if col not in ns_df.columns]\n",
    "    if missing_columns:\n",
    "        raise ValueError(\n",
    "            f\"Missing required columns in NatureSense data: {missing_columns}\"\n",
    "        )\n",
    "\n",
    "    # Check if required columns exist in country distribution data\n",
    "    required_cols = [\"na_entity_id\", \"total_company_locations\"]\n",
    "    missing_cols = [col for col in required_cols if col not in country_dist.columns]\n",
    "    if missing_cols:\n",
    "        raise ValueError(\n",
    "            f\"Missing required columns in country distribution data: {missing_cols}\"\n",
    "        )\n",
    "\n",
    "    # Check if required columns exist in country priors data\n",
    "    required_cols = [\"country_code\"] + nature_sense_evidence_columns  # Only check for columns we're using\n",
    "    missing_cols = [col for col in required_cols if col not in country_priors.columns]\n",
    "    if missing_cols:\n",
    "        raise ValueError(\n",
    "            f\"Missing required columns in country priors data: {missing_cols}\"\n",
    "        )\n",
    "\n",
    "    # Process country priors data\n",
    "    result = process_company_evidence(\n",
    "        company_data=ns_df,\n",
    "        country_dist=country_dist,\n",
    "        country_priors=country_priors,\n",
    "        evidence_columns=nature_sense_evidence_columns,\n",
    "        k=k,  # 20 locations threshold for including country priors in company analysis\n",
    "    )\n",
    "\n",
    "    print(\"shape non nans\",result[~result[\"sensitive_locations_posterior\"].isna()].shape, 'total rows', result.shape[0])\n",
    "    # Filter for non-null sensitive_locations_posterior\n",
    "    valid_locations = result #[~result[\"sensitive_locations_posterior\"].isna()]\n",
    "\n",
    "    return valid_locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = process_company_evidence(\n",
    "        company_data=companies_evidences.head(100),\n",
    "        country_dist=assets_guestimates,\n",
    "        country_priors=naturesense_country,\n",
    "        evidence_columns=naturesense_metrics,\n",
    "        k=10,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
