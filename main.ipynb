{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "from configparser import ConfigParser\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from utils.gcp_tools import (\n",
    "    get_git_branch,\n",
    "    last_day_of_month,\n",
    "    run_query,\n",
    "    save_results,\n",
    "    write_df_to_bq,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Read your config.ini\n",
    "config = ConfigParser()\n",
    "config.read(\"config.ini\")\n",
    "\n",
    "# 2. Grab the section name from the ENVIRONMENT env‑var\n",
    "ENVIRONMENT = os.environ.get(\"ENVIRONMENT\", \"dev\")\n",
    "\n",
    "# 3. Fetch that section (this yields a SectionProxy)\n",
    "cfg = config[ENVIRONMENT]\n",
    "\n",
    "PROJECT_ID = cfg[\"PROJECT_ID\"]\n",
    "BQ_DATASET = cfg[\"BQ_DATASET\"]\n",
    "ALD = cfg[\"ALD\"]\n",
    "ASSET_COUNTS_GUESTIMATES = cfg[\"ASSET_COUNTS_GUESTIMATES\"]\n",
    "NATURESENSE_COUNTRY = cfg[\"NATURESENSE_COUNTRY\"]\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data() -> tuple:\n",
    "    \"\"\"Load all required data from BigQuery using consistently formed SQL queries.\"\"\"\n",
    "    # ALD\n",
    "    query_ald = f\"\"\"\n",
    "    SELECT \n",
    "        na_entity_id, \n",
    "        entity_isin, \n",
    "        entity_name,\n",
    "        priority_asset,\n",
    "        asset_type_id,\n",
    "        sensitive_locations, \n",
    "        biodiversity_importance, \n",
    "        high_ecosystem_integrity, \n",
    "        decline_in_ecosystem_integrity,\n",
    "        physical_water_risk, \n",
    "        ecosystem_services_provision_importance, \n",
    "        proximity_to_protected_areas, \n",
    "        proximity_to_kbas,\n",
    "        species_rarity_weighted_richness, \n",
    "        species_threat_abatement, \n",
    "        species_threat_abatement_marine, \n",
    "        proximity_to_mangroves,\n",
    "        ecosystem_intactness_index, \n",
    "        biodiversity_intactness_index, \n",
    "        ocean_health_index, \n",
    "        trend_in_ecosystem_intactness_index,\n",
    "        deforestation_hotspots, \n",
    "        water_availability, \n",
    "        water_pollution, \n",
    "        drought, \n",
    "        riverine_flood, \n",
    "        coastal_flood, \n",
    "        cumulative_impact_on_oceans, \n",
    "        critical_areas_for_biodiversity_and_ncp, \n",
    "        areas_of_importance_for_biodiversity_and_climate,\n",
    "        in_water_scarcity\n",
    "    FROM {ALD};\n",
    "    \"\"\"\n",
    "    logging.info(\"Loading data from %s\", ALD)\n",
    "    ald = run_query(query_ald)\n",
    "\n",
    "    # Asset counts guestimates\n",
    "    query_assets_guestimates = f\"\"\"\n",
    "    SELECT\n",
    "        *\n",
    "    FROM {ASSET_COUNTS_GUESTIMATES};\n",
    "    \"\"\"\n",
    "    logging.info(\"Loading data from %s\", ASSET_COUNTS_GUESTIMATES)\n",
    "    assets_guestimates = run_query(query_assets_guestimates)\n",
    "\n",
    "    # NatureSense country level\n",
    "    query_ns_country = f\"\"\"\n",
    "    SELECT\n",
    "        *\n",
    "    FROM {NATURESENSE_COUNTRY};\n",
    "    \"\"\"\n",
    "    logging.info(\"Loading data from %s\", NATURESENSE_COUNTRY)\n",
    "    naturesense_country = run_query(query_ns_country)\n",
    "\n",
    "    return ald, assets_guestimates, naturesense_country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loading data from na-datalake.production_ready_access_layer.naturesense_solved_assets\n",
      "INFO:root:Using local service account: na-datalake-6f5a94599e1b 1.json\n",
      "INFO:root:Loading data from na-datalake.production_ready_access_layer.guestimator_latest\n",
      "INFO:root:Loading data from na-datalake.production_ready_access_layer.naturesense_country_level\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "ald, assets_guestimates, naturesense_country = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate companies evidences, i.e., aggregate ALD to company\n",
    "ald[\"material_asset\"] = ~ald[\"asset_type_id\"].isin([11, 12]).astype(bool)\n",
    "# ald[\"in_water_scarcity\"] = (\n",
    "#     (ald[\"water_availability\"] > 0.6) & (ald[\"material_asset\"] == True)\n",
    "# ).astype(bool\n",
    "\n",
    "ald_counts = (\n",
    "    ald.groupby(\"na_entity_id\")\n",
    "    .agg(\n",
    "        assets_count=(\"na_entity_id\", \"count\"),\n",
    "        priority_assets_count=(\"priority_asset\", \"sum\"),\n",
    "        material_assets_count=(\"material_asset\", \"sum\"),\n",
    "        in_water_scarcity_count=(\"in_water_scarcity\", \"sum\"),\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "ald_counts[\"priority_assets_percentage\"] = round(\n",
    "    (ald_counts[\"priority_assets_count\"] / ald_counts[\"assets_count\"]) * 100, 3\n",
    ")\n",
    "\n",
    "ald_counts[\"in_water_scarcity_percentage\"] = round(\n",
    "    (ald_counts[\"in_water_scarcity_count\"] / ald_counts[\"assets_count\"]) * 100, 3\n",
    ")\n",
    "\n",
    "ald_subset = ald[ald[\"material_asset\"] == True]\n",
    "\n",
    "naturesense_metrics = [\n",
    "    \"sensitive_locations\",\n",
    "    \"biodiversity_importance\",\n",
    "    \"high_ecosystem_integrity\",\n",
    "    \"decline_in_ecosystem_integrity\",\n",
    "    \"physical_water_risk\",\n",
    "    \"ecosystem_services_provision_importance\",\n",
    "    \"proximity_to_protected_areas\",\n",
    "    \"proximity_to_kbas\",\n",
    "    \"species_rarity_weighted_richness\",\n",
    "    \"species_threat_abatement\",\n",
    "    \"species_threat_abatement_marine\",\n",
    "    \"proximity_to_mangroves\",\n",
    "    \"ecosystem_intactness_index\",\n",
    "    \"biodiversity_intactness_index\",\n",
    "    \"ocean_health_index\",\n",
    "    \"trend_in_ecosystem_intactness_index\",\n",
    "    \"deforestation_hotspots\",\n",
    "    \"water_availability\",\n",
    "    \"water_pollution\",\n",
    "    \"drought\",\n",
    "    \"riverine_flood\",\n",
    "    \"coastal_flood\",\n",
    "    \"cumulative_impact_on_oceans\",\n",
    "    \"critical_areas_for_biodiversity_and_ncp\",\n",
    "    \"areas_of_importance_for_biodiversity_and_climate\",\n",
    "    ]\n",
    "\n",
    "ald_averages = (\n",
    "    ald_subset.groupby(\"na_entity_id\")\n",
    "    .agg(\n",
    "        **{\n",
    "            f\"{col}\": (col, lambda x: round(x.mean(skipna=True), 3))\n",
    "            for col in naturesense_metrics\n",
    "        }\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "companies_evidences = ald_counts.merge(\n",
    "    ald_averages, on=\"na_entity_id\", how=\"left\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate global median for each metric in naturesense_metrics\n",
    "ald_global_median = {\n",
    "    metric: round(ald_subset[metric].median(skipna=True), 3)\n",
    "    for metric in naturesense_metrics\n",
    "}\n",
    "\n",
    "# Convert to list of values\n",
    "ald_global_median = [float(val) for val in ald_global_median.values()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Country_priors.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import logging\n",
    "import re\n",
    "from typing import List, Union, Tuple\n",
    "from tqdm import tqdm\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.ERROR, format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_posterior(\n",
    "    evidence: Union[pd.Series, list, float, int],\n",
    "    prior: float,\n",
    "    sample_size: Union[pd.Series, list, float, int],\n",
    "    k: Union[float, int],\n",
    ") -> Union[pd.Series, float, None]:\n",
    "    \"\"\"\n",
    "    Compute posterior scores by combining entity-specific evidence with a global prior\n",
    "    based on sample size.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Ensure evidence and sample_size are converted to NumPy arrays for consistency\n",
    "        if isinstance(evidence, (float, int)):\n",
    "            evidence = np.array([evidence], dtype=float)\n",
    "        elif isinstance(evidence, list):\n",
    "            evidence = np.array(evidence, dtype=float)\n",
    "        elif isinstance(evidence, pd.Series):\n",
    "            evidence = evidence.to_numpy(dtype=float)\n",
    "        elif isinstance(evidence, (np.ndarray, np.int64, np.float64)):\n",
    "            evidence = np.array(evidence, dtype=float)\n",
    "\n",
    "        else:\n",
    "            logger.error(f\"Invalid type for evidence: {type(evidence)}\")\n",
    "            return None\n",
    "\n",
    "        if isinstance(sample_size, (float, int)):\n",
    "            sample_size = np.array([sample_size], dtype=float)\n",
    "        elif isinstance(sample_size, list):\n",
    "            sample_size = np.array(sample_size, dtype=float)\n",
    "        elif isinstance(sample_size, pd.Series):\n",
    "            sample_size = sample_size.to_numpy(dtype=float)\n",
    "        elif isinstance(sample_size, (np.ndarray, np.int64, np.float64)):\n",
    "            sample_size = np.array(sample_size, dtype=float)\n",
    "        else:\n",
    "            logger.error(f\"Invalid type for sample_size: {type(sample_size)}\")\n",
    "            return None\n",
    "\n",
    "        if not isinstance(k, (int, float)) or k < 0:\n",
    "            logger.error(\"k must be a positive number\")\n",
    "            return None\n",
    "\n",
    "        # Convert inputs to NumPy arrays for consistency\n",
    "        evidence = np.atleast_1d(np.asarray(evidence, dtype=float))\n",
    "        sample_size = np.atleast_1d(np.asarray(sample_size, dtype=float))\n",
    "\n",
    "        # Handle edge cases for arrays\n",
    "        if k == 0:\n",
    "            if np.all(sample_size == 0):\n",
    "                # Return zeros with same shape as input\n",
    "                zeros = np.zeros_like(evidence)\n",
    "                return float(zeros[0]) if zeros.size == 1 else pd.Series(zeros)\n",
    "            return float(evidence[0]) if evidence.size == 1 else pd.Series(evidence)\n",
    "        elif np.all(sample_size == 0):\n",
    "            # Return prior with same shape as input\n",
    "            priors = np.full_like(evidence, prior)\n",
    "            return float(priors[0]) if priors.size == 1 else pd.Series(priors)\n",
    "\n",
    "        # Compute weights safely, avoiding NaN by ensuring effective_k is never zero\n",
    "        adapted_k = np.minimum(sample_size / k, 1)\n",
    "        w_i = np.where(sample_size == 0, 1, adapted_k)\n",
    "\n",
    "        # Compute posterior\n",
    "        theta_i = (w_i * evidence + (1 - w_i) * prior).round(3)\n",
    "\n",
    "        # Return a float if the input was scalar, else return a Series\n",
    "        return float(theta_i[0]) if theta_i.size == 1 else pd.Series(theta_i)\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during posterior computation: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def get_company_countries(company_row: pd.Series) -> List[str]:\n",
    "    \"\"\"\n",
    "    Extract valid ISO country codes from company row where asset count > 0.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    company_row : pd.Series\n",
    "        Row containing company's country distribution\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List[str]\n",
    "        List of valid ISO country codes with positive asset counts\n",
    "    \"\"\"\n",
    "    iso_pattern = re.compile(r\"^[A-Z0-9]{3}$\")\n",
    "    return [c for c in company_row.index if iso_pattern.match(c) and company_row[c] > 0]\n",
    "\n",
    "\n",
    "def calculate_country_prior(\n",
    "    company_row: pd.Series,\n",
    "    country_priors: pd.DataFrame,\n",
    "    evidence_columns: Union[str, List[str]],\n",
    "    country_codes: List[str],\n",
    "    entity_id: str,\n",
    ") -> List[float]:\n",
    "    \"\"\"\n",
    "    Calculate the country-weighted prior for a company for one or multiple geospatial columns.\n",
    "    Uses vectorized operations to compute weighted averages for all evidence columns at once.\n",
    "    Returns None for each column if there are no valid weights.\n",
    "    \"\"\"\n",
    "    # Ensure evidence_columns is a list\n",
    "    if isinstance(evidence_columns, str):\n",
    "        evidence_columns = [evidence_columns]\n",
    "\n",
    "    # Obtain guestimated countries with company presence\n",
    "    company_countries = get_company_countries(company_row)\n",
    "\n",
    "    # Check which countries have country avg prior available\n",
    "    available_countries = [c for c in company_countries if c in country_codes]\n",
    "    missing_countries = set(company_countries) - set(country_codes)\n",
    "\n",
    "    if missing_countries:\n",
    "        logger.info(\n",
    "            f\"NA_entity_id {entity_id} has assets in countries missing from priors: {missing_countries}. \"\n",
    "            \"These will be excluded from the weighted average calculation.\"\n",
    "        )\n",
    "\n",
    "    if company_row.empty:\n",
    "        logger.warning(f\"No country distribution found for NA_entity_id {entity_id}\")\n",
    "        return [None] * len(evidence_columns)\n",
    "\n",
    "    # Get the priors matrix for available countries and evidence columns\n",
    "    priors_matrix = country_priors[\n",
    "        country_priors[\"country_code\"].isin(available_countries)\n",
    "    ]\n",
    "    priors_matrix = priors_matrix.set_index(\"country_code\")[evidence_columns]\n",
    "\n",
    "    # Create weights array for available countries\n",
    "    weights = pd.Series(\n",
    "        {country: company_row[country] for country in available_countries}\n",
    "    )\n",
    "\n",
    "    # If no weights or all weights are zero, return None for each column\n",
    "    if not available_countries or weights.sum() == 0:\n",
    "        return [None] * len(evidence_columns)\n",
    "\n",
    "    # Normalize weights and ensure index alignment\n",
    "    weights = weights / weights.sum()\n",
    "    weights = weights.reindex(priors_matrix.index)\n",
    "\n",
    "    # Compute weighted average for all columns at once using matrix multiplication\n",
    "    weighted_priors = weights.dot(priors_matrix)\n",
    "\n",
    "    # Convert to list and handle any NaN values\n",
    "    return [float(val) if pd.notnull(val) else None for val in weighted_priors]\n",
    "\n",
    "\n",
    "def calculate_effective_k(k: float, estimated_material_assets_count: int, material_assets_count: int) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the effective k value based on various scenarios.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    k : float\n",
    "        Original k value. If < 1, interpreted as proportion of estimated_material_assets_count\n",
    "    estimated_material_assets_count : int\n",
    "        Estimated total number of company assets in the world\n",
    "    material_assets_count : int\n",
    "        Actual number of discovered company assets\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Adjusted k value\n",
    "    \"\"\"\n",
    "    # If k is proportional, convert to absolute number\n",
    "    # Ensure effective_k is at least 1 if estimated_material_assets_count is greater than 0\n",
    "    if k < 1:\n",
    "        effective_k = max(1, np.ceil(k * estimated_material_assets_count)) if estimated_material_assets_count > 0 else 0\n",
    "    else:\n",
    "        effective_k = k\n",
    "\n",
    "    # If we found more assets than estimated locations, update our estimate\n",
    "    if estimated_material_assets_count != 0 and material_assets_count > estimated_material_assets_count:\n",
    "        estimated_material_assets_count = material_assets_count\n",
    "        \n",
    "    # If estimated total locations is less than k, adjust k down\n",
    "    if estimated_material_assets_count < effective_k:\n",
    "        effective_k = estimated_material_assets_count\n",
    "\n",
    "    return float(effective_k)\n",
    "\n",
    "\n",
    "def no_guestimates_adjust_priors_and_k(weighted_priors: Union[List[float], None], effective_k: float, k: float, default_priors: Union[List[float], None]) -> Tuple[List[float], float]:\n",
    "    \"\"\"\n",
    "    Adjust priors and effective_k for companies with no prior and few assets.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    weighted_priors : Union[List[float], None]\n",
    "        List of weighted priors, may contain None values, or be None itself\n",
    "    effective_k : float\n",
    "        Current effective k value\n",
    "    k : float\n",
    "        Original k value\n",
    "    default_priors : Union[List[float], None]\n",
    "        List of default prior values from global medians\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[List[float], float]\n",
    "        Adjusted weighted priors and effective k value\n",
    "    \"\"\"\n",
    "    if weighted_priors is None or any(p is None for p in weighted_priors):\n",
    "        if effective_k < k:\n",
    "            if weighted_priors is None:\n",
    "                weighted_priors = default_priors.copy()\n",
    "            else:\n",
    "                weighted_priors = [\n",
    "                    default_priors[i] if p is None else p \n",
    "                    for i, p in enumerate(weighted_priors)\n",
    "            ]\n",
    "            effective_k = k\n",
    "    return weighted_priors, effective_k\n",
    "\n",
    "\n",
    "def process_company_evidence(\n",
    "    company_data: pd.DataFrame,\n",
    "    country_dist: pd.DataFrame,\n",
    "    country_priors: pd.DataFrame,\n",
    "    evidence_columns: List[str],\n",
    "    k: Union[int, float] = 10,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Process company evidence using country distribution and priors.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    company_data : pd.DataFrame\n",
    "        Company-level data containing:\n",
    "        - na_entity_id: Company identifier\n",
    "        - material_assets_count: Number of total assets\n",
    "        - evidence_columns: Columns containing evidence to process\n",
    "    country_dist : pd.DataFrame\n",
    "        Country distribution data containing:\n",
    "        - na_entity_id: Company identifier\n",
    "        - total_company_locations: Optional total locations count\n",
    "        - One column per country code with number of assets\n",
    "    country_priors : pd.DataFrame\n",
    "        Country-level priors containing:\n",
    "        - country_code: ISO alpha-3 country code\n",
    "        - columns matching evidence_columns with prior values\n",
    "    evidence_columns : List[str]\n",
    "        List of column names in company_data to process\n",
    "    k : int or float, optional\n",
    "        If k > 1: interpreted as absolute number of asset locations\n",
    "        If k ≤ 1: interpreted as proportion of company's total locations\n",
    "        Default is 10.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame with original company data plus new posterior columns\n",
    "    \"\"\"\n",
    "    logger.info(\"Starting company evidence processing\")\n",
    "\n",
    "    try:\n",
    "        # Validate input company_data\n",
    "        company_data_required_cols = [\"na_entity_id\", \"material_assets_count\", *evidence_columns]\n",
    "\n",
    "        if not all(col in company_data.columns for col in company_data_required_cols):\n",
    "            missing_cols = set(company_data_required_cols) - set(company_data.columns)\n",
    "            raise ValueError(\n",
    "                f\"Missing required columns in company_data: {missing_cols}\"\n",
    "            )\n",
    "\n",
    "        # Validate input country_dist\n",
    "        if \"na_entity_id\" not in country_dist.columns:\n",
    "            raise ValueError(\"country_dist must contain 'na_entity_id' column\")\n",
    "\n",
    "        ## Check if country_dist has any country codes not in country_priors\n",
    "        country_codes = country_priors[\"country_code\"].tolist()\n",
    "        country_dist_basic_cols = [\"na_entity_id\", \"entity_isin\", \"entity_name\", \"factset_entity_name\", \"factset_coverage_name\", \"total_company_locations\", \"number_material_assets\", \"primary_sector\", \"partition_date\"]\n",
    "        invalid_country_codes = [col for col in country_dist.columns if col not in country_codes and col not in country_dist_basic_cols]\n",
    "\n",
    "        if invalid_country_codes:\n",
    "            raise ValueError(f\"country_dist contains country codes not found in country_priors: {invalid_country_codes}\")\n",
    "\n",
    "        ## Sum of the assets listed under each country code\n",
    "        country_dist_copy = country_dist.copy()\n",
    "        country_cols = [col for col in country_dist.columns if col in country_codes]\n",
    "        country_assets_sum = country_dist_copy[country_cols].sum(axis=1)\n",
    "        country_dist_copy[\"total_company_locations\"] = country_assets_sum\n",
    "\n",
    "        # Validate input country_priors\n",
    "        available_evidence = [\n",
    "            col for col in evidence_columns if col in country_priors.columns\n",
    "        ]\n",
    "        missing_evidence = set(evidence_columns) - set(available_evidence)\n",
    "\n",
    "        if missing_evidence:\n",
    "            raise ValueError(\n",
    "                f\"Following evidence columns not found in country_priors: {missing_evidence}\"\n",
    "            )\n",
    "\n",
    "        # Create output DataFrame\n",
    "        result_df = company_data.copy()\n",
    "\n",
    "        # Add posterior columns to result_df, initialized as copies of original columns\n",
    "        for col in evidence_columns:\n",
    "            result_df[f\"{col}_posterior\"] = result_df[col]\n",
    "\n",
    "        # Keep track of missing entity ids\n",
    "        missing_entity_ids = []\n",
    "        \n",
    "        # Process each company\n",
    "        for entity_id in tqdm(\n",
    "            result_df[\"na_entity_id\"].unique(), desc=\"Implementing NatureSense Priors\"\n",
    "        ):\n",
    "            # Initialize\n",
    "            material_assets_count = int(company_data.loc[\n",
    "                company_data[\"na_entity_id\"] == entity_id, \"material_assets_count\"\n",
    "            ].iloc[0])\n",
    "        \n",
    "            estimated_material_assets_count = 0\n",
    "            \n",
    "            # If material_assets_count >= k don't adjust company evidence\n",
    "            if material_assets_count >= k:\n",
    "                continue\n",
    "            \n",
    "            # Get company evidence values for all columns\n",
    "            company_evidence = result_df.loc[\n",
    "                result_df[\"na_entity_id\"] == entity_id, evidence_columns\n",
    "            ].iloc[0]\n",
    "        \n",
    "            # Get company country distribution and estimated_material_assets_count\n",
    "            if entity_id not in country_dist_copy[\"na_entity_id\"].values:\n",
    "                missing_entity_ids.append(entity_id)\n",
    "                weighted_priors = [None] * len(evidence_columns)  # Initialize with correct length\n",
    "            else:\n",
    "                company_row = country_dist_copy[\n",
    "                    country_dist_copy[\"na_entity_id\"] == entity_id\n",
    "                ].iloc[0]\n",
    "        \n",
    "                estimated_material_assets_count = company_row[\"total_company_locations\"]\n",
    "        \n",
    "                # If both material_assets_count and estimated_material_assets_count are 0, set all evidence columns to None\n",
    "                if material_assets_count == 0 and estimated_material_assets_count == 0:\n",
    "                    for col in evidence_columns:\n",
    "                        result_df.loc[result_df[\"na_entity_id\"] == entity_id, f\"{col}_posterior\"] = np.nan\n",
    "                    continue\n",
    "                    \n",
    "                # Get weighted priors\n",
    "                weighted_priors = calculate_country_prior(\n",
    "                    company_row=company_row,\n",
    "                    country_priors=country_priors,\n",
    "                    evidence_columns=evidence_columns,\n",
    "                    country_codes=country_codes,\n",
    "                    entity_id=entity_id,\n",
    "                )\n",
    "                \n",
    "            # Calculate effective k\n",
    "            effective_k = calculate_effective_k(\n",
    "                k=k,\n",
    "                estimated_material_assets_count=estimated_material_assets_count,\n",
    "                material_assets_count=material_assets_count\n",
    "            )\n",
    "        \n",
    "            # Adjust priors and k if necessary\n",
    "            weighted_priors, effective_k = no_guestimates_adjust_priors_and_k(weighted_priors, effective_k, k, ald_global_median)\n",
    "            \n",
    "            # Now weighted_priors should never be None, but its elements might be\n",
    "            for idx, col in enumerate(evidence_columns):\n",
    "                if weighted_priors[idx] is None:  # Check individual prior\n",
    "                    continue\n",
    "                \n",
    "                # Get evidence value for this specific column\n",
    "                evidence_value = float(company_evidence[col])\n",
    "        \n",
    "                # Compute posterior using the corresponding prior\n",
    "                posterior = compute_posterior(\n",
    "                    evidence=evidence_value,\n",
    "                    prior=float(weighted_priors[idx]),\n",
    "                    sample_size=material_assets_count,\n",
    "                    k=effective_k,\n",
    "                )\n",
    "                # Store result\n",
    "                result_df.loc[result_df[\"na_entity_id\"] == entity_id, f\"{col}_posterior\"] = posterior\n",
    "\n",
    "        if missing_entity_ids:\n",
    "            logger.warning(\n",
    "                f\"{len(missing_entity_ids)} companies were not found in company country distribution data\"\n",
    "            )\n",
    "\n",
    "        logger.info(\"Completed company evidence processing\")\n",
    "        return result_df\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in process_company_evidence: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = process_company_evidence(\n",
    "        company_data=companies_evidences.head(100),\n",
    "        country_dist=assets_guestimates.drop(columns=\"HKG\"),\n",
    "        country_priors=naturesense_country,\n",
    "        evidence_columns=naturesense_metrics,\n",
    "        k=10,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
