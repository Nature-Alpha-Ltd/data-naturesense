{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "from configparser import ConfigParser\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from utils.gcp_tools import (\n",
    "    get_git_branch,\n",
    "    last_day_of_month,\n",
    "    run_query,\n",
    "    save_results,\n",
    "    write_df_to_bq,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Read your config.ini\n",
    "config = ConfigParser()\n",
    "config.read(\"config.ini\")\n",
    "\n",
    "# 2. Grab the section name from the ENVIRONMENT envâ€‘var\n",
    "ENVIRONMENT = os.environ.get(\"ENVIRONMENT\", \"dev\")\n",
    "\n",
    "# 3. Fetch that section (this yields a SectionProxy)\n",
    "cfg = config[ENVIRONMENT]\n",
    "\n",
    "PROJECT_ID = cfg[\"PROJECT_ID\"]\n",
    "BQ_DATASET = cfg[\"BQ_DATASET\"]\n",
    "ALD = cfg[\"ALD\"]\n",
    "ASSET_COUNTS_GUESTIMATES = cfg[\"ASSET_COUNTS_GUESTIMATES\"]\n",
    "NATURESENSE_COUNTRY = cfg[\"NATURESENSE_COUNTRY\"]\n",
    "MASTER_TABLE = cfg[\"MASTER_TABLE\"]\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data() -> tuple:\n",
    "    \"\"\"Load all required data from BigQuery using consistently formed SQL queries.\"\"\"\n",
    "    # ALD\n",
    "    query_ald = f\"\"\"\n",
    "    SELECT \n",
    "        na_entity_id, \n",
    "        entity_isin, \n",
    "        entity_name,\n",
    "        priority_asset,\n",
    "        asset_type_id,\n",
    "        sensitive_locations, \n",
    "        biodiversity_importance, \n",
    "        high_ecosystem_integrity, \n",
    "        decline_in_ecosystem_integrity,\n",
    "        physical_water_risk, \n",
    "        ecosystem_services_provision_importance, \n",
    "        proximity_to_protected_areas, \n",
    "        proximity_to_kbas,\n",
    "        species_rarity_weighted_richness, \n",
    "        species_threat_abatement, \n",
    "        species_threat_abatement_marine, \n",
    "        proximity_to_mangroves,\n",
    "        ecosystem_intactness_index, \n",
    "        biodiversity_intactness_index, \n",
    "        ocean_health_index, \n",
    "        trend_in_ecosystem_intactness_index,\n",
    "        deforestation_hotspots, \n",
    "        water_availability, \n",
    "        water_pollution, \n",
    "        drought, \n",
    "        riverine_flood, \n",
    "        coastal_flood, \n",
    "        cumulative_impact_on_oceans, \n",
    "        critical_areas_for_biodiversity_and_ncp, \n",
    "        areas_of_importance_for_biodiversity_and_climate,\n",
    "        in_water_scarcity\n",
    "    FROM {ALD};\n",
    "    \"\"\"\n",
    "    logging.info(\"Loading data from %s\", ALD)\n",
    "    ald = run_query(query_ald)\n",
    "\n",
    "    # Asset counts guestimates\n",
    "    query_assets_guestimates = f\"\"\"\n",
    "    SELECT\n",
    "        *\n",
    "    FROM {ASSET_COUNTS_GUESTIMATES};\n",
    "    \"\"\"\n",
    "    logging.info(\"Loading data from %s\", ASSET_COUNTS_GUESTIMATES)\n",
    "    assets_guestimates = run_query(query_assets_guestimates)\n",
    "\n",
    "    # NatureSense country level\n",
    "    query_ns_country = f\"\"\"\n",
    "    SELECT\n",
    "        *\n",
    "    FROM {NATURESENSE_COUNTRY};\n",
    "    \"\"\"\n",
    "    logging.info(\"Loading data from %s\", NATURESENSE_COUNTRY)\n",
    "    naturesense_country = run_query(query_ns_country)\n",
    "\n",
    "    # ISIN master table\n",
    "    query_master_table = f\"\"\"\n",
    "    SELECT\n",
    "        na_entity_id,\n",
    "        entity_isin,\n",
    "        entity_name\n",
    "    FROM {MASTER_TABLE};\n",
    "    \"\"\"\n",
    "    logging.info(\"Loading data from %s\", MASTER_TABLE)\n",
    "    master_table = run_query(query_master_table)\n",
    "\n",
    "    return ald, assets_guestimates, naturesense_country, master_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loading data from na-datalake.production_ready_access_layer.naturesense_solved_assets\n",
      "INFO:root:Loading data from na-datalake.production_ready_access_layer.guestimator_latest\n",
      "INFO:root:Loading data from na-datalake.production_ready_access_layer.naturesense_country_level\n",
      "INFO:root:Loading data from na-datalake.production_ready_access_layer.isin_master_table_latest\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "ald, assets_guestimates, naturesense_country, master_table = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate companies evidences, i.e., aggregate ALD to company\n",
    "ald[\"material_asset\"] = ~ald[\"asset_type_id\"].isin([11, 12]).astype(bool)\n",
    "# ald[\"in_water_scarcity\"] = (\n",
    "#     (ald[\"water_availability\"] > 0.6) & (ald[\"material_asset\"] == True)\n",
    "# ).astype(bool\n",
    "\n",
    "ald_counts = (\n",
    "    ald.groupby(\"na_entity_id\")\n",
    "    .agg(\n",
    "        assets_count=(\"na_entity_id\", \"count\"),\n",
    "        priority_assets_count=(\"priority_asset\", \"sum\"),\n",
    "        material_assets_count=(\"material_asset\", \"sum\"),\n",
    "        in_water_scarcity_count=(\"in_water_scarcity\", \"sum\"),\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "ald_counts[\"priority_assets_percentage\"] = round(\n",
    "    (ald_counts[\"priority_assets_count\"] / ald_counts[\"assets_count\"]) * 100, 3\n",
    ")\n",
    "\n",
    "ald_counts[\"in_water_scarcity_percentage\"] = round(\n",
    "    (ald_counts[\"in_water_scarcity_count\"] / ald_counts[\"assets_count\"]) * 100, 3\n",
    ")\n",
    "\n",
    "ald_subset = ald[ald[\"material_asset\"] == True]\n",
    "\n",
    "naturesense_metrics = [\n",
    "    \"sensitive_locations\",\n",
    "    \"biodiversity_importance\",\n",
    "    \"high_ecosystem_integrity\",\n",
    "    \"decline_in_ecosystem_integrity\",\n",
    "    \"physical_water_risk\",\n",
    "    \"ecosystem_services_provision_importance\",\n",
    "    \"proximity_to_protected_areas\",\n",
    "    \"proximity_to_kbas\",\n",
    "    \"species_rarity_weighted_richness\",\n",
    "    \"species_threat_abatement\",\n",
    "    \"species_threat_abatement_marine\",\n",
    "    \"proximity_to_mangroves\",\n",
    "    \"ecosystem_intactness_index\",\n",
    "    \"biodiversity_intactness_index\",\n",
    "    \"ocean_health_index\",\n",
    "    \"trend_in_ecosystem_intactness_index\",\n",
    "    \"deforestation_hotspots\",\n",
    "    \"water_availability\",\n",
    "    \"water_pollution\",\n",
    "    \"drought\",\n",
    "    \"riverine_flood\",\n",
    "    \"coastal_flood\",\n",
    "    \"cumulative_impact_on_oceans\",\n",
    "    \"critical_areas_for_biodiversity_and_ncp\",\n",
    "    \"areas_of_importance_for_biodiversity_and_climate\",\n",
    "    ]\n",
    "\n",
    "ald_averages = (\n",
    "    ald_subset.groupby(\"na_entity_id\")\n",
    "    .agg(\n",
    "        **{\n",
    "            f\"{col}\": (col, lambda x: round(x.mean(skipna=True), 3))\n",
    "            for col in naturesense_metrics\n",
    "        }\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "companies_evidences = ald_counts.merge(\n",
    "    ald_averages, on=\"na_entity_id\", how=\"left\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate global median for each metric in naturesense_metrics\n",
    "ald_global_median = {\n",
    "    metric: round(ald_subset[metric].median(skipna=True), 3)\n",
    "    for metric in naturesense_metrics\n",
    "}\n",
    "\n",
    "# Convert to list of values\n",
    "ald_global_median = [float(val) for val in ald_global_median.values()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Country_priors.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import logging\n",
    "import re\n",
    "from typing import List, Union, Tuple\n",
    "from tqdm import tqdm\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.ERROR, format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# process_company_evidence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_data=companies_evidences\n",
    "country_dist=assets_guestimates\n",
    "country_priors=naturesense_country\n",
    "evidence_columns=naturesense_metrics\n",
    "k=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate input company_data\n",
    "company_data_required_cols = [\"na_entity_id\", \"material_assets_count\", *evidence_columns]\n",
    "\n",
    "if not all(col in company_data.columns for col in company_data_required_cols):\n",
    "    missing_cols = set(company_data_required_cols) - set(company_data.columns)\n",
    "    raise ValueError(\n",
    "        f\"Missing required columns in company_data: {missing_cols}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate input country_dist\n",
    "if \"na_entity_id\" not in country_dist.columns:\n",
    "    raise ValueError(\"country_dist must contain 'na_entity_id' column\")\n",
    "\n",
    "# Check if country_dist has any country codes not in country_priors\n",
    "country_codes = country_priors[\"country_code\"].tolist()\n",
    "country_dist_basic_cols = [\n",
    "            \"na_entity_id\",\n",
    "            \"entity_isin\",\n",
    "            \"entity_name\",\n",
    "            \"factset_entity_name\",\n",
    "            \"factset_coverage_name\",\n",
    "            \"estimated_assets_count\",\n",
    "            \"estimated_material_assets_count\",\n",
    "            \"material_assets_types\",\n",
    "            \"primary_sector\",\n",
    "            \"partition_date\",\n",
    "        ]\n",
    "invalid_country_codes = [col for col in country_dist.columns if col not in country_codes and col not in country_dist_basic_cols]\n",
    "\n",
    "if invalid_country_codes:\n",
    "    raise ValueError(f\"country_dist contains country codes not found in country_priors: {invalid_country_codes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate input country_priors\n",
    "available_evidence = [\n",
    "    col for col in evidence_columns if col in country_priors.columns\n",
    "]\n",
    "missing_evidence = set(evidence_columns) - set(available_evidence)\n",
    "\n",
    "if missing_evidence:\n",
    "    raise ValueError(\n",
    "        f\"Following evidence columns not found in country_priors: {missing_evidence}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output DataFrame\n",
    "result_df = company_data.copy()\n",
    "\n",
    "# Add posterior columns to result_df, initialized as copies of original columns\n",
    "posterior_cols = [f\"{col}_posterior\" for col in evidence_columns]\n",
    "result_df[posterior_cols] = result_df[evidence_columns]\n",
    "\n",
    "# Add estimated_material_assets_count column initialized with 0\n",
    "result_df[\"estimated_material_assets_count\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep track of missing entity ids\n",
    "missing_entity_ids = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process each company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_id = result_df[\"na_entity_id\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize\n",
    "material_assets_count = int(company_data.loc[\n",
    "    company_data[\"na_entity_id\"] == entity_id, \"material_assets_count\"\n",
    "].iloc[0])\n",
    "\n",
    "estimated_material_assets_count = 0\n",
    "\n",
    "material_assets_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sensitive_locations                                 0.895\n",
       "biodiversity_importance                             0.261\n",
       "high_ecosystem_integrity                            0.888\n",
       "decline_in_ecosystem_integrity                      0.516\n",
       "physical_water_risk                                 0.932\n",
       "ecosystem_services_provision_importance             0.449\n",
       "proximity_to_protected_areas                        0.954\n",
       "proximity_to_kbas                                   0.129\n",
       "species_rarity_weighted_richness                    0.078\n",
       "species_threat_abatement                            0.278\n",
       "species_threat_abatement_marine                     0.000\n",
       "proximity_to_mangroves                              0.000\n",
       "ecosystem_intactness_index                          0.431\n",
       "biodiversity_intactness_index                       0.888\n",
       "ocean_health_index                                    NaN\n",
       "trend_in_ecosystem_intactness_index                 0.516\n",
       "deforestation_hotspots                              0.000\n",
       "water_availability                                  0.938\n",
       "water_pollution                                     1.000\n",
       "drought                                             0.384\n",
       "riverine_flood                                      0.134\n",
       "coastal_flood                                       0.055\n",
       "cumulative_impact_on_oceans                         0.409\n",
       "critical_areas_for_biodiversity_and_ncp             0.349\n",
       "areas_of_importance_for_biodiversity_and_climate    0.550\n",
       "Name: 0, dtype: float64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get evidence values for all columns - do this for all companies\n",
    "company_evidence = result_df.loc[\n",
    "    result_df[\"na_entity_id\"] == entity_id, evidence_columns\n",
    "].iloc[0]\n",
    "company_evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_priors = [None] * len(evidence_columns)\n",
    "weighted_priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_row = country_dist[\n",
    "    country_dist[\"na_entity_id\"] == entity_id\n",
    "].iloc[0]\n",
    "company_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimated_material_assets_count = company_row[\"estimated_material_assets_count\"]\n",
    "estimated_material_assets_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## calculate_country_prior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_company_countries(company_row: pd.Series) -> List[str]:\n",
    "    \"\"\"\n",
    "    Extract valid ISO country codes from company row where asset count > 0.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    company_row : pd.Series\n",
    "        Row containing company's country distribution\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List[str]\n",
    "        List of valid ISO country codes with positive asset counts\n",
    "    \"\"\"\n",
    "    iso_pattern = re.compile(r\"^[A-Z0-9]{3}$\")\n",
    "    return [c for c in company_row.index if iso_pattern.match(c) and company_row[c] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain guestimated countries with company presence\n",
    "company_countries = get_company_countries(company_row)\n",
    "company_countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check which countries have country avg prior available\n",
    "available_countries = [c for c in company_countries if c in country_codes]\n",
    "available_countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_countries = set(company_countries) - set(country_codes)\n",
    "missing_countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "if missing_countries:\n",
    "    logger.info(\n",
    "        f\"NA_entity_id {entity_id} has assets in countries missing from priors: {missing_countries}. \"\n",
    "        \"These will be excluded from the weighted average calculation.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "if company_row.empty:\n",
    "    logger.warning(f\"No country distribution found for NA_entity_id {entity_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[None] * len(evidence_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the priors matrix for available countries and evidence columns\n",
    "priors_matrix = country_priors[\n",
    "    country_priors[\"country_code\"].isin(available_countries)\n",
    "]\n",
    "priors_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "priors_matrix = priors_matrix.set_index(\"country_code\")[evidence_columns]\n",
    "priors_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create weights array for available countries\n",
    "weights = pd.Series(\n",
    "    {country: company_row[country] for country in available_countries}\n",
    ")\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize weights and ensure index alignment\n",
    "weights = weights / weights.sum()\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = weights.reindex(priors_matrix.index)\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute weighted average for all columns at once using matrix multiplication\n",
    "weighted_priors = weights.dot(priors_matrix)\n",
    "weighted_priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_priors = [float(val) if pd.notnull(val) else None for val in weighted_priors]\n",
    "weighted_priors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## calculate_effective_k()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimated_material_assets_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.ceil(k * estimated_material_assets_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "effective_k = k\n",
    "effective_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we found more assets than estimated locations, update our estimate\n",
    "if estimated_material_assets_count != 0 and material_assets_count > estimated_material_assets_count:\n",
    "    estimated_material_assets_count = material_assets_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimated_material_assets_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If estimated total locations is less than k, adjust k down\n",
    "if estimated_material_assets_count < effective_k:\n",
    "    effective_k = estimated_material_assets_count\n",
    "\n",
    "effective_k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## no_guestimates_adjust_priors_and_k()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_priors = ald_global_median\n",
    "default_priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "if weighted_priors is None or any(p is None for p in weighted_priors):\n",
    "    if effective_k < k:\n",
    "        if weighted_priors is None:\n",
    "            weighted_priors = default_priors.copy()\n",
    "        else:\n",
    "            weighted_priors = [\n",
    "                default_priors[i] if p is None else p \n",
    "                for i, p in enumerate(weighted_priors)\n",
    "            ]\n",
    "        effective_k = k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "effective_k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compute_posterior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a DataFrame of evidences\n",
    "company_evidence_df = pd.DataFrame(\n",
    "                company_evidence[evidence_columns].astype(float).values.reshape(1, -1),\n",
    "                columns=evidence_columns,\n",
    "                index=[0],\n",
    "            )\n",
    "company_evidence_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame of priors\n",
    "weighted_priors_df = pd.DataFrame(\n",
    "    {col: [weighted_priors[idx]] for idx, col in enumerate(evidence_columns)},\n",
    "    index=[0]\n",
    ")\n",
    "weighted_priors_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_posterior(\n",
    "    evidences: pd.DataFrame,\n",
    "    priors: pd.DataFrame,\n",
    "    sample_size: int,\n",
    "    k: float,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute posterior scores by combining entity-specific evidences with priors\n",
    "    based on sample size.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    evidences : pd.DataFrame\n",
    "        DataFrame containing evidence values for each metric\n",
    "    priors : pd.DataFrame\n",
    "        DataFrame containing prior values for each metric\n",
    "    sample_size : int\n",
    "        Number of assets/samples for the company\n",
    "    k : float\n",
    "        Threshold value for sample size adjustment\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.Series\n",
    "        Series containing posterior values for each metric, indexed by metric names\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Check if both inputs are DataFrames\n",
    "        if not isinstance(evidences, pd.DataFrame) or evidences.empty:\n",
    "            logger.error(\"evidences must be a non-empty DataFrame\")\n",
    "            return None\n",
    "\n",
    "        if not isinstance(priors, pd.DataFrame) or priors.empty:\n",
    "            logger.error(\"priors must be a non-empty DataFrame\")\n",
    "            return None\n",
    "\n",
    "        if not isinstance(k, (int, float)) or k < 0:\n",
    "            logger.error(\"k must be a positive number\")\n",
    "            return None\n",
    "\n",
    "        # Handle edge cases\n",
    "        if k == 0:\n",
    "            if sample_size == 0:\n",
    "                return pd.DataFrame(0, index=evidences.index, columns=evidences.columns)\n",
    "            return evidences.iloc[0]\n",
    "        elif sample_size == 0:\n",
    "            return priors.iloc[0]\n",
    "\n",
    "        # Compute weights safely, avoiding NaN by ensuring effective_k is never zero\n",
    "        adapted_k = min(sample_size / k, 1)\n",
    "        w_i = 1 if sample_size == 0 else adapted_k\n",
    "\n",
    "        # Compute posterior using vectorized operations\n",
    "        theta_i = w_i * evidences + (1 - w_i) * priors\n",
    "\n",
    "        return theta_i.iloc[0]\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during posterior computation: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_company_evidences = pd.DataFrame(\n",
    "            {\n",
    "                \"sensitive_locations\": [0.895],\n",
    "                \"biodiversity_importance\": [0.261],\n",
    "                \"high_ecosystem_integrity\": [0.888],\n",
    "                \"decline_in_ecosystem_integrity\": [0.516],\n",
    "                \"physical_water_risk\": [0.932],\n",
    "                \"ecosystem_services_provision_importance\": [0.449],\n",
    "            },\n",
    "            index=[0],\n",
    "        )\n",
    "\n",
    "t_weighted_priors_df = pd.DataFrame(\n",
    "            {\n",
    "                \"sensitive_locations\": [0.567],\n",
    "                \"biodiversity_importance\": [0.409],\n",
    "                \"high_ecosystem_integrity\": [0.750],\n",
    "                \"decline_in_ecosystem_integrity\": [0.101],\n",
    "                \"physical_water_risk\": [0.543],\n",
    "                \"ecosystem_services_provision_importance\": [0.567]\n",
    "            },\n",
    "            index=[0],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute posteriors for all columns at once\n",
    "result = compute_posterior(\n",
    "    evidences=t_company_evidences,\n",
    "    priors=t_weighted_priors_df,\n",
    "    sample_size=5,\n",
    "    k=10,\n",
    ")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute posteriors for all columns at once\n",
    "posteriors = compute_posterior(\n",
    "    evidences=company_evidence_df,\n",
    "    priors=weighted_priors_df,\n",
    "    sample_size=material_assets_count,\n",
    "    k=effective_k,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posteriors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Populate result_df with posterior values\n",
    "result_df.loc[\n",
    "    result_df[\"na_entity_id\"] == entity_id, posterior_cols\n",
    "] = posteriors.to_numpy(dtype=\"float64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Round posterior values to 3 decimals\n",
    "result_df[posterior_cols] = result_df[posterior_cols].round(3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
